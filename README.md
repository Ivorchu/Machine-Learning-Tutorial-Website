# 摺積神經網路
###### 2021 陽明交大資訊人才培訓筆記 by Ivor Chu
###### tags: `機器學習`

摺積神經網路的特性
---
**視覺感知過程**
某些神經元對於某些光線的角度特別敏感，因此各種不同的神經元負責各自的角度，而每個神經元再處理各自的角度時，他們的上層神經元會把這些角度組合起來，上層神經元會匯集下層的神經元來判斷現在在畫面中看到的是什麼形狀。神經元有階層的特性，下層會處理較小的特徵，上層再組合起來形成一個更高階的新特徵。
![](https://i.imgur.com/k5WpppM.png)

**稀疏連接**
影像由很多像素所組成，而相鄰像素的關係會比較強，也就是說一張照片中，若一個像素是皮膚色，則左右相鄰的像素也有很高的機率是皮膚色，相對較遠的像素比較難預測，稱為區域特徵。在學習上，因為已經知道某些像素的關係會大於某些像素，因此只需要處理關係較高的像素(用來合成較大的特徵)而不用浪費在關係較低的像素上。學習區域特徵比較容易學到畫面上像素之間的關係，同時也可以讓模型變簡單。稀疏連接可以被視為減少神經元參數並限制模型的複雜度，若一個神經元直接去學習整張道片，很容易發生過度擬合。
![](https://i.imgur.com/IdOc8xp.png)

**參數共享**
一個神經元是負責檢查一小塊區與有沒有某種特徵，若有則被激活，但因為區域特徵很有可能會重複出現，就像一張畫面中可能會有很多直線與斜線，因此只要有一個神經元學到直線的特徵，就可以拿他的參數來在整張影像上找這種特徵，不用讓很多個神經元重複學習此特徵，以達到參數共享。
![](https://i.imgur.com/FOdEheO.png)

摺積與反摺積
---
**數學定義**
在數學上，摺積就拿是兩個訊號來做摺積後，會得到一個新的訊號，假設有兩個訊號X()和W()，當X()不動而把W()對折(加一個負號)，再把W()滑動到要做摺積的X()上，移動的過程叫kernel，當X()與W()有接觸到的點，就把兩個值乘再一起，就會得到一個新的值S(t)。
![](https://i.imgur.com/0vJUK5H.png)

* 張量input: (影像數量, 影像高度, 影像寬度, kernel 數量 channel-->RGB就是3)
* 摺積核kernel: (摺積核高度, 摺積核寬度, channel)
* Padding: 額外補多少邊界
* Stride: kernel一次要走多大步
摺積與反摺積互為反操作，因為摺積為線性運算，摺積運算可以用矩陣表示，成上反矩陣就可以達到反操做。
![](https://i.imgur.com/fyq9xrM.gif)

摺積可以被視為一個矩陣相乘的運算，只需要將摺積核與影像矩陣展開至下圖所示，相乘後得到的一維矩陣再合併成二維矩陣就可以達到摺積。
![](https://i.imgur.com/FJiIDg9.png)
反摺積也是一樣，只是乘上輸出就可以得到影像矩陣。
![](https://i.imgur.com/W4bZZbB.png)


摺積神經網路架構
---
**結構**
摺積神經網路的架構如下圖所示，今天若有ㄧ個影像，你設定的kernel摺積核會掃過整張影像來做摺積，並得到一個新的矩陣稱為Feature Map特徵圖，有幾個kernel(channel)就會得到幾張Feature Map(張量)，而kernel可以被看作為局部特徵掃過整張影像這件事就是在做參數共享，把得到的所有Feature Map都疊起來後會變成一個立方體，這時再做池化Pooling或叫subsampling來把整個立方體切塊並只取一小塊來傳入下一層神經元中再做一次摺積，這個過程雖然會把面積縮小，但深度仍舊一樣，重複上述的步驟直到不能再做摺積或是達到事先設定的次數後，再將其拉長成一維矩陣並當作輸入值來丟入最原始的全連結神經網路中。
![](https://i.imgur.com/2VjsYTX.png)

**摺積層**
輸入的張量的厚度必須要和接下來的每一個摺積核一樣厚，否則放不上去，例如圖示中的Channel厚度為3(RGB)，所以下一步裡的所有摺積核厚度都一定要是3，再下一步中的摺積核厚度D一定要是上一步張量的數量，因為每一步都會把張量疊起來形成一個更厚的立方體。摺積核代表著一個區域特徵，就如同視覺感知過程中，神經元會由小特徵組合成大特徵，而每個摺積核代表著一個小特徵，此時就會出現一個特性叫冗餘特徵Reductant Feature，也就是有很多特徵本質上是一樣的特徵，只是轉換一個角度而已，就像一條斜線本質上就是一條直線，只是傾斜而已，他們的資訊量是差不多的。
![](https://i.imgur.com/fdgo0Ex.png)
![](https://i.imgur.com/k2hayLH.png)

**池化層**
在進行摺積時總要把特徵圖的面積縮小，這時就可以用一些方法來把一些多餘的資訊捨去，並留下較為重要的資訊。當張量疊成一個立方體後，就可以將它切割成預先設定好的份數，並在每一份中取出最大值來當作新立方體的參數，稱為Max-Pooling，透過這個方式可以減少計算量。
![](https://i.imgur.com/zsnMETN.png)
Max-Pooling可以只讓輸出最強的神經元通過，以過濾最強的訊號，這樣的方法犧牲了空間的資訊，他只會考慮特徵的有無而不會考慮特徵確切出現的位子，例如今天使用這種方法只會告訴你這張圖有一條斜線，而不會告訴你這條斜線在摺積核中或整張圖中(做完無數次摺積後空間資訊還是會被過濾掉)的哪裡。不過此方法可以增強輸入特徵的不變性，就像今天有一隻貓耳朵，不管他怎麼旋轉或位移，此方法都能找出此特徵。

**神經元感受域**
摺積會將原本很大一張影像逐步變窄，由寬淺變成窄深，這件事情會讓表徵能力增加，也就是描述資料的特性，這樣可以逐步地讓更多神經元來代表更重要的特徵(池化時會省略不重要的特徵)，如下圖所示，更重要的特徵就會逐步地用更多神經元來描述他，最後會得到一串很長的神經元來描述整張畫面，就可以拿這些資訊來做學習。
![](https://i.imgur.com/p2Ur69U.png)
![](https://i.imgur.com/AidZIC3.png)
![](https://i.imgur.com/KAbotjA.png)
下圖為摺積神經網路辨識影像的範例
![](https://i.imgur.com/dOzrXah.png)

深度殘差網路
---
**跳躍結構**
隨著神經網路加深，學習容量增加，模型變得更複雜參數也變得更多，這樣會導致更難訓練，因為你的假設空間會變得大很多，想要找到一個理想的模型跟大海撈針一樣，所以就要定義假設空間來簡化神經網路結構。今天若輸入的資料比較簡單，則可以暫時刪掉幾層，這時模型就會變得比較簡單，在學習的過程中自己把神經元層拔掉稱為跳躍，想要跳過的神經元在學習時參數都會被調成0，所以他可以直接跳過這些層，透過這個方式就可以搭建出非常深的神經網路，模型空間和複雜度又會隨著訓練過程去調整。
![](https://i.imgur.com/imbM2TR.png)

**殘差神經網路**
在數學公式上，從上層傳下來的數值叫X，經過某層的轉換叫F(X)，轉換完的叫H(X)，而H(X)來自於F(X)，在這一層有轉換F(X)+X或者沒有轉換X，如果你想要跳過此層則將此層的F(X)設為0，也就是學到的東西設為0，如果有用到此層，則給予其一些權重F(X)，這樣也可以寫成下圖所示的遞迴式，最後可以整理成任兩層L與l之間的關係為他們中間用了幾個F(X)的總和，因為他們是個線性的關係，再加上一堆非線性的關係(中間使用的F(X))，就可以到達下一層，這樣可以達到平滑的梯度傳遞。
![](https://i.imgur.com/Fwsub8S.png)

**平滑梯度傳遞**
在做層和層傳遞時都會要乘上一個激活函數才可以從這一層跳到下一層，但在經過激活函數的微分常常會造成梯度消失，就像Sigmoid只有一些值才有梯度，而Relu只有在大於0時才有梯度，但因為今天任兩層的關係是線性，所以在很深的一層做微分可以找到很淺的一層(比較靠近輸出)。在數學式上，目前的這一層可以被複製到很深的那一層，而最靠近輸出的激活函數的梯度大家都看得到，就不會有不見或越來越小的問題，這件事就會上訓練變得很平滑，同時也可以不用顧慮消失的問題去訓練很深的神經網路。
![](https://i.imgur.com/rjy8MCe.png)


其他筆記
---
* [人工智慧與機器學習](https://hackmd.io/@ivorchu/BkCeyJydd)
* [深度學習](https://hackmd.io/@ivorchu/rJ3DPsgud)
* [卷積神經網路](https://hackmd.io/@ivorchu/SkIIoLkOu)
* [遞迴神經網路](https://hackmd.io/@ivorchu/Hkoaoildu)
# 人工智慧與機器學習
###### 2021 陽明交大資訊人才培訓 by Ivor Chu
###### tags: `機器學習`

機器學習
---
**何謂機器學習**
機器能從經驗或是資料中學習，而後在某個給定的任務或問題上衡量學習進展得好壞。例如在騎腳踏車這件事上，只要騎的越多，越能掌握騎腳踏車的技巧。

**考試學習**
* 訓練資料
    * 參考書的好壞-->念錯書怎麼辦？考化學第一章結果讀成第二章？
* 問題與任務
    * 小考考好就一定代表大考會考得好嗎？
    * 狂刷題？背答案？對學習有幫助嗎?
* 衡量標準
    * 選擇題？申論題？
    * 在練習中要如何衡量每次練習得好壞？分數？

機器學習問題類型
---
**監督式學習 Supervised Learning**
* 訓練資料
    * 資料與標籤
    * 一筆資料對到一組答案
* 目標
    * 學習資料與標籤中的關係
    * 選擇題與答案之間的關係？
    * 線性回歸 Linear Regretion
        * 一個y對到一個x
        * 透過回歸模型給定資料來預測答案
    * 給你一張貓的照片，叫你判別是貓還是狗

**非監督式學習**
* 訓練資料
    * 僅包和資料本身，並沒有標籤
    * 申論題並沒有一個標準答案
    * 給你一堆影像，卻沒有人告訴你那是貓還是狗
* 目標
    * 學習資料間所隱含的關係
    * 你跟親戚長得很像-->你和他是兄弟？
    * 社交網路終將興趣相同的人歸類成一群-->分群

**增強式學習**
* 訓練資料
    * 來自於一系列與環境互動的行動與獎勵回饋-->最像人的學習模式
    * 學騎腳踏車
        * 一剛開始跌倒-->環境給予負面回饋
        * 多騎了10公尺-->環境給與正面回饋
    * 利用每次訓練時得到的經驗
* 目標
    * 學習如何選擇動作並累積最大的獎勵
    * 騎腳踏車
        * 學習一系列的身體動作-->騎得越遠越好
        * AlphaGo
            * 下棋為一系列的過程
            * 每次都往回報最多的地方走
    * 從環境的回饋來衡量行為得好壞
        * 負面回饋不一定不好？
        * 正面回饋不一定好？

機器學習流程
---
![](https://i.imgur.com/qZHx8Pd.png)
**真實模型**
機器學習的目的是想要了解兩個模型中的真實關係，尋找是否存在一個完美的公式當資料輸入時可以準確的輸出相對應的資量。我們不知道這個真實模型長怎樣，但是我們有很多比資料相對應到正確的標籤，這些訓練資料都是由真實模型所產生的。

**假設集合**
在還不知道答案前人們通常都會有很多假設，就像古代人看到地平線就假設地球是平的。在尋找真實模型前須先製造大量假設真實模型的樣子，稱為假設集合。深度學習就是一種假設集合，只是用網路的方式來呈現。

**學習演算法**
學習演算法會拿預先準備的資料在假設集合中一個一個挑出最好的假設，這個好或壞稱為訓練誤差。訓練誤差計算當我從假設集合中抽出的模型，到底和真實模型像不像，目標是找到一個和真實模型最像的假設模型，也就是找到誤差或相異最小的模型，稱為預測模型。

**訓練資料**
訓練資料並沒有一定的類型，任何東西都可以是訓練資料，但從真實模型中產生出來的資料一定會有誤差，就像是霧裡看花一樣，因為資料有誤差的關係，所以我們看不到真實模型，稱為噪音。若從錯誤的模型中搜集到資料，像是想要找到A模型，資料卻始終從B模型產生，我們將永遠找不到A模型，同時，若資料也搜集的不夠完全，像是一個外星人降落在台灣，周圍都是黃種人，就認定地球上都是黃種人，稱為偏誤。

**機器學習目標**
在假設集合中找到一個最接近真實模型的預測模型，若假設集合很大，就會要找很久，若訓練資料很差，就會影響模型的精準度。若預測模型對於資料輸入輸出的表現和真實模型一樣，就可以取代真實模型，因為他們不管做什麼事情都一樣，稱為常規化。但訓練資料上表現好，並不代表所有資料上真的好，稱為過度擬合。

線性模型與特徵轉換
---
![](https://i.imgur.com/zZyLzcB.png)
**線性函數**
當給予很多不同的W，就可以產生很多函數，也稱為假設空間。雖然真實模型多半很複雜，我們還是用線性函數來創造假設模型，因為線性函數已經有很多現成的數學工具，而將複雜的真實模型分成很多簡單的線性模型稱為特徵轉換。

**線性假設空間**
在一個平面上，任一個點，也就是資料，都可以寫成一個函數，這樣我們就可以算兩個函數近不近，稱為函數空間。

**特徵轉換**
真實空間往往都是非線性，特徵轉換就是把非線性的問題線性化。就像一群人站位臨臨亂亂，但有一個人突然叫大家排直。很高維的資料有時無法用線性來表示，這時就要用特徵轉換ø將他們的空間關係進行調整，把資料轉換到一個可以讓這些資料的排列看起來避叫線性畫的新空間，在用線性模型處理來找到一個線性模型近似真實模型。這個過程有點像把複雜的問題拆成小問題，並且用現有的工具來解決這些簡化過的小問題，通常都是透過經驗來人為調整特徵轉換方法。
![](https://i.imgur.com/ibOVxrw.png)

過度擬合與常規化
---
**過度擬合**
當給予的資料過少，同時又要找到一個完美的模型，往往都會出問題，就像是被下整張考卷的答案，雖然考了滿分，卻還是不會內容。當一個假設模型訓練的狀況很好，就會被演算法挑出來，每一筆訓練資料都答對，但是我們希望再給予任何資料的情況下表現也要和真實模型一樣好，太相信訓練資料就會導致以為真實模型是訓練資料給出來的樣子，稱為過度擬合。
* 模型容易受資料影響
    * 過於複雜的模型太難進行學習-->簡單模型優先-->更有彈性
* 資料有噪音或變異很大
    * 避免學到不是真實模型該有的關係-->清理與預處理資料
![](https://i.imgur.com/Okn8ttD.png)

**常規化**
當模型很彈性，太容易相信資料，避免學習演算法挑到太複雜的架設(容易過度擬合)，就要用到常規化。常規化來限制假設空間中能挑選的範圍，因為每個點可以用函數f(x)=WX1+WX2...來表示，限制W的範圍來限制函數彎曲的幅度，進而挑到簡單的模型。
![](https://i.imgur.com/LhduePp.png)

特徵學習
---
**特徵**
特徵本質上是一個函數，他能擷取出輸入的某些特性，神經網路在學習完後會了解到這筆資料有某些特性，像是貓就是有鬍鬚、狗就是有耳朵。特徵能看作是一個函數因為當你把資料丟入某特徵函數，他會回傳相對應特徵的值，而某些神經元會隱含著特徵的函數。
![](https://i.imgur.com/JPAc0Qo.png)

**好的特徵**
特徵的好壞需要搭配你定義的問題，像是當你想把貓跟老虎分開，因為他們都有鬍鬚，有沒有鬍鬚這個特徵自然就不是一個好的特徵。好的特徵需要購有代表性，像是分類貓和狗，有沒有鬍鬚就是一個很好的特徵，因為你可以把這兩類分得很開。好的特徵也需要能容忍變異，像是當你想要比較一個人的身高，但當你讓比較高的人倒下時，原本身高比較高的人會比身高比較低的人還矮。

**人工定義特徵**
資料轉換函數ø就是一種你定義的特徵，人工定義的特徵需要依靠你的經驗與研究來對不同的問題進行分類。

**自動學習特徵**
定義層次結構可以更有效的重複利用已學習到的特徵，組合成更複雜的特徵。像是一個三角形的鼻子可能是由一種三角形構成的，而這個三角形可能又是由某些線段構成的。而組成是透過一組學到的特定權重值W來取決要組合哪些特徵，像是一個鼻子可能是由0.1的這種線、0.2的另一種線以及0.5的另一種線等等來組成，權重越高越有可能被用來組合，也代表從資料中學到的模式。
![](https://i.imgur.com/jq7lIBw.jpg)

表徵學習
---
**表徵**
表徵是一種假設性的，能夠表示外在現實的內在認知象徵，或著是一種可以讓心智過程得以利用的內在認知象徵。像是你是怎麼描述貓這個概念，可能是橘色、有眼睛、有鬍鬚等等，你的認知中的貓是長怎樣，因該要有怎麼樣的特徵。而以不同的方式，通常是以較為簡單、好處理理解的方式，來表示一筆資料，稱為資料表徵。像是1164x7899直接乘很複雜，但我們可以先將數字表徵成1164=2x2x3x3x97, 7899=3x2633來簡化問題，並計算2x2x3x3x97x3x2633，這樣的問題就變得簡單很多。但在表徵資料的同時也要確保表徵能維持原始資料原來的特性，因為表徵是依據資料所構建的。

**分散式表徵**
假設常用的字有3000個，貓和狗分別排在編號100和101，並以0和1在向量中用1來表示他的所在位置，稱為One-hot Encoding。但在這裡貓和狗並沒有距離性(字跟字距離0)，無法表示出貓和狗與比人還近，做完內積後值還是0。不過如果用分散式表徵，將所有資量重新分布，並儲存資料之間的距離，像是表示貓與狗的距離很近(貓與狗相對長得比較像)，但與人比較遠，讓資料有空間性，也可以算內積，就可以求兩筆資料的相似性。
![](https://i.imgur.com/wYaYO4i.png)
例如下圖中存在一些單字的向量，可以發現相似屬性或是意義的單字的距離會比較近，並且可以用向量中的距離來表示。這也可以被表示為在某一個單字往哪個方向走多遠就可以碰到哪個單字，透過這個空間慨念就可以顯現資料的關聯性，並且具有表徵的意義。
![](https://i.imgur.com/leAQdZD.png)

**表徵學習方法**
* 主成份分析
* 自動編碼器
* 深度學習

自動編碼器
---
自動編碼器會根據W的值，將輸入的資料壓縮編碼成比較好處理的表徵，而這些表徵也能被解碼還原成接近原始的資料。學習到的表徵需要隱含足夠的輸入資料的資訊，也就是說若學習到的表徵很接近原始資料，就可以取代原始資料，在處理和儲存上都更節省，要用的時候可以隨時解碼表徵。使用層次結構能逐層的利用學習到的特徵，重新表達輸入資料，形成新的表徵。也就是說在層次結構中，每一層會產生很多參考上一層的特徵做出來的表徵。深層表徵可以處理很多複雜的問因為他可以包含很多細節，也就是透過特徵學習到的表徵。
![](https://i.imgur.com/w86OmRX.png)

其他筆記
---
* [人工智慧與機器學習](https://hackmd.io/@ivorchu/BkCeyJydd)
* [深度學習](https://hackmd.io/@ivorchu/rJ3DPsgud)
* [卷積神經網路](https://hackmd.io/@ivorchu/SkIIoLkOu)
* [遞迴神經網路](https://hackmd.io/@ivorchu/Hkoaoildu)

